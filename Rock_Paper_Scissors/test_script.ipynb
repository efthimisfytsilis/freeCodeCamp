{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Training\n",
    "The agent periodically updates its strategy based on recent gameplay experiences, thus adapting to changes in opponent strategies over time.  \n",
    "An interesting article explaining the Q-Table algorithm is found in [this](https://towardsdatascience.com/math-of-q-learning-python-code-5dcbdc49b6f6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results: {'p1': 561, 'p2': 47, 'tie': 392}\n",
      "Player 1 win rate: 92.26973684210526%\n",
      "Final results: {'p1': 830, 'p2': 166, 'tie': 4}\n",
      "Player 1 win rate: 83.33333333333334%\n",
      "Final results: {'p1': 497, 'p2': 46, 'tie': 457}\n",
      "Player 1 win rate: 91.52854511970534%\n",
      "Final results: {'p1': 467, 'p2': 280, 'tie': 253}\n",
      "Player 1 win rate: 62.51673360107095%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.51673360107095"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rps_game import *\n",
    "from rps_agent import *\n",
    "\n",
    "agent = QLearning()\n",
    "set_agent(agent)\n",
    "\n",
    "play(player, quincy, 1000)\n",
    "play(player, mrugesh, 1000)\n",
    "play(player, kris, 1000)\n",
    "play(player, abbey, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Learning\n",
    "Multiple instances of the agent are trained with different opponents and their outcomes are combined through a majority voting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_choose_action(agent_ensemble, state):\n",
    "    # Voting mechanism: each agent votes, and the action with the majority wins\n",
    "    votes = [agent.get_action(state) for agent in agent_ensemble]\n",
    "    majority_vote = max(set(votes), key=votes.count)\n",
    "    return majority_vote\n",
    "\n",
    "def player(prev_play, opponent_history = []):\n",
    "    global agent_ensemble, opponents\n",
    "\n",
    "    for i, opponent in enumerate(opponents):\n",
    "\n",
    "        if agent_ensemble[i].last_action is None or prev_play == '':\n",
    "            action = random.choice(agent_ensemble[i].actions)\n",
    "            agent_ensemble[i].last_action = action\n",
    "            agent_ensemble[i].step += 1\n",
    "            return action\n",
    "\n",
    "        state = (agent_ensemble[i].last_action, prev_play) \n",
    "\n",
    "    return ensemble_choose_action(agent_ensemble, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rps_game import *\n",
    "from rps_agent_ens import *\n",
    "\n",
    "opponents = [quincy, mrugesh, kris, abbey]\n",
    "agent_ensemble = [QLearningAgent() for _ in range(len(opponents))]\n",
    "\n",
    "for i, opponent in enumerate(opponents):\n",
    "    agent_ensemble[i].train(opponent, num_episodes=6000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(player, quincy, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
